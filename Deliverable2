# download data (-q is the quiet mode)
! wget -q https://www.dropbox.com/s/lhb1awpi769bfdr/test.csv?dl=1 -O test.csv
! wget -q https://www.dropbox.com/s/gudb5eunj700s7j/train.csv?dl=1 -O train.csv
# This is pretty much irrelevant - simply downloading the data


import pandas as pd

Xy_train = pd.read_csv('train.csv', engine='python')
X_train = Xy_train.drop(columns=['price_rating'])
y_train = Xy_train[['price_rating']]

print('traning', len(X_train))
Xy_train.price_rating.hist()

X_test = pd.read_csv('test.csv', engine='python')
testing_ids = X_test.Id
print('testing', len(X_test))
# This is all just setting up the data as variables in order to run the algorithms



# model training and tuning
import numpy as np
from sklearn.compose import ColumnTransformer
from sklearn.datasets import fetch_openml
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from xgboost.sklearn import XGBClassifier
# This is just importing the needed classes and modules for the code

np.random.seed(0)
# This is setting the random numbers to be predictable

numeric_features = ['bedrooms', 'review_scores_location', 'accommodates', 'beds']
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])
# This accounts for missing values and completes them

categorical_features = [
  'property_type', 'is_business_travel_ready', 'room_type', ]
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])
# This accounts for missing values and completes them as well as
# the onehot encoder which encodes the categorical features as a numeric array

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])
# This allows different columns to be transformed seperately

regr = Pipeline(steps=[('preprocessor', preprocessor),
                      ('regressor', XGBClassifier(
                          objective='multi:softmax', seed=1))])
# This is the implementation of the model - XGBClassifier in this case - where
# model is run and the model set up for the data is now referred to as 'regr'


X_train = X_train[[*numeric_features, *categorical_features]]
X_test = X_test[[*numeric_features, *categorical_features]]
# This is the setup of the data into arrays that can be used with the model for
# training and testing

# `__` denotes attribute
# (e.g. regressor__n_estimators means the `n_estimators` param for `regressor`
#  which is our xgb)
param_grid = {
    'preprocessor__num__imputer__strategy': ['mean'],
    'regressor__n_estimators': [50, 100],
    'regressor__max_depth':[10, 20]
}
# This is the setup for the different parameters the model is going to
# experiment with. In this case it is going to be run with two different
# n estimators and two different max depths. This is where the model tuning
# can be done a little easier - setting up the different parameters to all run
# in conjunction with one another.

grid_search = GridSearchCV(
    regr, param_grid, cv=5, verbose=3, n_jobs=2,
    scoring='accuracy')
grid_search.fit(X_train, y_train)
# This takes the model and runs it with a cross validation and on two seperate
# processes. The highest scored parameter set is going to be used to fit the
# model to the data.

print('best score {}'.format(grid_search.best_score_))
# This outputs the highest score reached with the parameter set given.

# Prediction & generating the submission file
y_pred = grid_search.predict(X_test)
pd.DataFrame(
    {'Id': testing_ids, 'price_rating':y_pred}).to_csv('sample_submission.csv', index=False)
# This is the most simple portion of the code - simply outputting the calculated
# classifications of the predictions.
